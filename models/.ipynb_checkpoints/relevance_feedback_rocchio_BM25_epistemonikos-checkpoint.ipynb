{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relevance Feedback for Epistemonikos Dataset using Relevance Feedback Rocchio and BM25 \n",
    "- For these experiments we used relevance feedback Rocchio and BM25 on the same used Epsitemonikos dataset to compare the results with active learning framework. \n",
    "- Documents ids, titles and abstracts were indexed in ElasticSearch to retrieve documents more efficiently when using neither BM25 or Roccio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T14:57:42.534325Z",
     "start_time": "2019-12-10T14:57:40.564622Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "STOPWORDS = list(stopwords.words('english'))\n",
    "from elasticsearch import Elasticsearch\n",
    "import json \n",
    "\n",
    "# start elastic search session \n",
    "es = Elasticsearch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T15:30:45.214364Z",
     "start_time": "2019-12-10T15:30:45.210593Z"
    }
   },
   "outputs": [],
   "source": [
    "def text_processing(doc_init):\n",
    "    document = re.sub(r'\\W', ' ', doc_init)\n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document)\n",
    "    document = re.sub(r'\\d+', ' ', document)\n",
    "    document = re.sub(r'[\\µ\\β\\ε\\χ2\\χ²\\δ\\å\\⁸\\α]', ' ', document)\n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "    document = re.sub(r'^b\\s+', '', document)\n",
    "    document = document.lower()    \n",
    "    return document.strip()\n",
    "\n",
    "def average_precision(lista):\n",
    "    precisions = []\n",
    "    cum = 1 \n",
    "    for i,x in enumerate(lista, start=1):\n",
    "        if x ==1:\n",
    "            precisions.append(cum/i)   \n",
    "            cum +=1         \n",
    "    return sum(precisions)/cum\n",
    "\n",
    "def last_rel(y):\n",
    "    lastrel = ''.join([str(x) for x in y]).rindex('1')\n",
    "    return lastrel/len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T15:17:42.331767Z",
     "start_time": "2019-12-10T15:17:42.216304Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "DATASET_DIR = '' # insert path where all files were downloaded\n",
    "\n",
    "df_episte = pd.read_csv('{}/datasets/Epistemonikos_dataset.csv'.format(DATASET_DIR), sep=';')\n",
    "df_episte.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T15:18:27.791383Z",
     "start_time": "2019-12-10T15:18:22.274702Z"
    }
   },
   "outputs": [],
   "source": [
    "# filter same matrices as active learning framework to make them comparable \n",
    "max_docs = 2200\n",
    "min_docs = 5 \n",
    "\n",
    "matrices = [m for m in list(df_episte.matrix.unique()) if len(df_episte[df_episte.matrix == m]) < max_docs and len(df_episte[df_episte.matrix == m]) > min_docs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T15:30:52.891702Z",
     "start_time": "2019-12-10T15:30:52.879369Z"
    }
   },
   "outputs": [],
   "source": [
    "df_episte = df_episte[df_episte['matrix'].isin(matrices)]\n",
    "df_episte.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## index data from queries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T15:18:39.487167Z",
     "start_time": "2019-12-10T15:18:39.484763Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('matrix_titles.json') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T15:18:44.906941Z",
     "start_time": "2019-12-10T15:18:42.029249Z"
    }
   },
   "outputs": [],
   "source": [
    "idx2query = {}\n",
    "query2idx = {}\n",
    "dict_episte_queries = {}\n",
    "\n",
    "count = 0 \n",
    "\n",
    "# clean data and create idx2query and query2idx dicts \n",
    "for idx in data:\n",
    "      \n",
    "    if data[idx] != None:    \n",
    "        idx2query[count] = idx\n",
    "        query2idx[idx] = count \n",
    "        relevant_documents = list(df_episte[df_episte['matrix']==idx].document)        \n",
    "        dict_episte_queries[idx] = {'title': data[idx], 'relevant_docs': relevant_documents}\n",
    "        count +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-05T14:32:07.623657Z",
     "start_time": "2019-12-05T14:32:07.619599Z"
    }
   },
   "source": [
    "## process data: \n",
    "Example ElasticSearch results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T15:19:01.240154Z",
     "start_time": "2019-12-10T15:19:01.235420Z"
    }
   },
   "outputs": [],
   "source": [
    "queries = [dict_episte_queries[idx]['title'].lower() for idx in dict_episte_queries]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevance Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T15:19:15.279050Z",
     "start_time": "2019-12-10T15:19:15.274492Z"
    }
   },
   "outputs": [],
   "source": [
    "class Query_result(object):\n",
    "    def __init__(self, pid, title_abstract, is_relevant = False):\n",
    "        self.pid = pid \n",
    "        self.title_abstract = title_abstract\n",
    "        self.is_relevant = False \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T15:19:16.028487Z",
     "start_time": "2019-12-10T15:19:16.023459Z"
    }
   },
   "outputs": [],
   "source": [
    "def search(query_terms, result_size):\n",
    "    \n",
    "    query = ' '.join(query_terms)\n",
    "    \n",
    "    \n",
    "    res = res= es.search(index='episte_index',body={'query':{'match':{'title_abstract': query}}}, size = result_size)\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "  \n",
    "    for item in res['hits']['hits']:\n",
    "        title_abstract = item['_source']['title_abstract']\n",
    "        \n",
    "        t = Query_result(item['_id'], title_abstract)\n",
    "        \n",
    "        result.append(t)\n",
    "    \n",
    "    return result \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T15:19:28.150035Z",
     "start_time": "2019-12-10T15:19:28.145483Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_feedback(query_result, query_id):\n",
    "    \n",
    "    for i, v in enumerate(query_result):\n",
    "        \n",
    "        # receive feedback from ground truth\n",
    "        if v.pid in dict_episte_queries[query_id]['relevant_docs']:\n",
    "            v.is_relevant = True \n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T15:19:28.756167Z",
     "start_time": "2019-12-10T15:19:28.754116Z"
    }
   },
   "outputs": [],
   "source": [
    "def regularize(string):\n",
    "    return [word for word in re.sub(r'[^a-zA-Z0-9_ ]', '', string).lower().strip().split()\\\n",
    "            if word not in STOPWORDS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T15:48:14.271032Z",
     "start_time": "2019-12-10T15:48:14.242311Z"
    }
   },
   "outputs": [],
   "source": [
    "def modify_query(query_result, query, ORIGIN_QUERY, alpha=0.25, beta=0.5):\n",
    "    N = len(query_result)\n",
    "    re_vectors, irre_vectors, doc_freq = [], [], defaultdict(set)\n",
    "    \n",
    "\n",
    "    for i, v in enumerate(query_result):\n",
    "        vector = defaultdict(int)\n",
    "  \n",
    "        terms = regularize(v.title_abstract) # terms: all terms in a document\n",
    "    \n",
    "        for term in terms:\n",
    "            doc_freq[term].add(i)\n",
    "            vector[term] += 1\n",
    "            \n",
    "        if v.is_relevant:\n",
    "            re_vectors.append(vector)  \n",
    "            \n",
    "        else:\n",
    "            irre_vectors.append(vector)         \n",
    "    \n",
    "    # After this loop, every vector, each representing a document,\n",
    "    # will store the tf-idf value for each term in this document\n",
    "    for vector in re_vectors + irre_vectors:\n",
    "        for term in vector:\n",
    "            vector[term] = math.log(1+vector[term], 10) * math.log(float(N)/len(doc_freq[term]), 10) * 10000\n",
    "    \n",
    "    # Rocchio Algorithm -- combine all relevant and irrelevant vectors\n",
    "    DR, DNR = len(re_vectors), len(irre_vectors)\n",
    "    new_vector = defaultdict(float)\n",
    "    \n",
    "    for vector in re_vectors:\n",
    "        for term in vector:\n",
    "            new_vector[term] += vector[term] * alpha / DR \n",
    "    \n",
    "    for vector in irre_vectors:\n",
    "        for term in vector:\n",
    "            new_vector[term] = max(0, new_vector[term] - vector[term] * beta / DNR)\n",
    "        \n",
    "    # Find (up to) 2 \"new\" terms in new_vector and add them to query terms\n",
    "    first, second, first_val, second_val = '', '', 0, 0\n",
    "    for term in new_vector:\n",
    "        if term not in query and new_vector[term] > 0: # pass terms that are already in query terms\n",
    "            weight = new_vector[term]\n",
    "            if weight > first_val:\n",
    "                first, first_val, second, second_val = term, weight, first, first_val\n",
    "            elif weight > second_val:\n",
    "                second, second_val = term, weight\n",
    "            else:\n",
    "                pass\n",
    " \n",
    "    if first: \n",
    "        query.append(first)\n",
    "\n",
    "    if second: \n",
    "        query.append(second)    \n",
    "        print(\"Augmenting by {}\".format(first + ' ' + second))\n",
    "    \n",
    "    new_vector_words = []\n",
    "    \n",
    "    for t in query:\n",
    "        if t not in ORIGIN_QUERY:\n",
    "            new_vector_words.append([t, new_vector[t]])\n",
    "    \n",
    "    # sort by score \n",
    "    new_vector_words.sort(key = lambda x: x[1], reverse = True)\n",
    "    \n",
    "    # obtain words to concat to query \n",
    "    concat_words = [x[0] for x in new_vector_words]\n",
    "    \n",
    "    query = ORIGIN_QUERY.rstrip('\\n').split() + concat_words\n",
    "\n",
    "    return query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# start relevance feedback (Rocchio) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T16:58:51.973829Z",
     "start_time": "2019-12-10T16:58:51.961415Z"
    }
   },
   "outputs": [],
   "source": [
    "def main_rocchio(): \n",
    "    \n",
    "    total_results  = {}\n",
    "    \n",
    "    query_ids = [idx2query[i] for i in range(len(queries))]\n",
    "    \n",
    "    # start_result_dict\n",
    "    for id_ in query_ids: \n",
    "        total_results[id_] = {'recall@10':[], \n",
    "                              'recall@20':[],\n",
    "                              'recall@30':[],\n",
    "                              'precision@10':[],\n",
    "                             'precision@20':[],\n",
    "                              'precision@30':[],\n",
    "                              'avg_prec':[],\n",
    "                              'lastrel%':[]\n",
    "                             }\n",
    "    \n",
    "    \n",
    "    for i in range(len(queries)):\n",
    "        \n",
    "        results = {}\n",
    "    \n",
    "        ORIGIN_QUERY = queries[i]\n",
    "\n",
    "        ORIGIN_QUERY = ORIGIN_QUERY.lower()  # Record original query terms\n",
    "\n",
    "        query = ORIGIN_QUERY.rstrip('\\n').split()\n",
    "\n",
    "        RESULT_SIZE = 20000 \n",
    "        \n",
    "        ITER = 20 \n",
    "\n",
    "        QUERY_ID = idx2query[i]\n",
    "\n",
    "        counter = 0\n",
    "        \n",
    "        recalls10 = []\n",
    "        recalls20 = []\n",
    "        recalls30 = []\n",
    "        \n",
    "        precisions10 = []\n",
    "        precisions20 = []\n",
    "        precisions30 = []\n",
    "        \n",
    "        avg_precisions = []\n",
    "        lastrels = []\n",
    "        \n",
    "\n",
    "        while True:\n",
    "\n",
    "            query_result = search(query, RESULT_SIZE)\n",
    "            \n",
    "            get_feedback(query_result, QUERY_ID)\n",
    "                      \n",
    "            pred = [1 if q.is_relevant else 0 for q in query_result]\n",
    "              \n",
    "            if sum(pred) > 0: \n",
    "            \n",
    "                recall10 = sum(pred[0:10])/len(dict_episte_queries[QUERY_ID]['relevant_docs'])\n",
    "                recall20 = sum(pred[0:20])/len(dict_episte_queries[QUERY_ID]['relevant_docs'])\n",
    "                recall30 = sum(pred[0:30])/len(dict_episte_queries[QUERY_ID]['relevant_docs'])\n",
    "\n",
    "                precision10 = sum(pred[0:10])/10\n",
    "                precision20 = sum(pred[0:20])/20\n",
    "                precision30 = sum(pred[0:30])/30\n",
    "\n",
    "                avg_precision = average_precision(pred)\n",
    "                lastrel = last_rel(pred)\n",
    "                \n",
    "                print('recall@10', recall10)\n",
    "                print('recall@20', recall20)\n",
    "                print('recall@30', recall30)\n",
    "                \n",
    "                print('precision@10', precision10)\n",
    "                print('precision@20', precision20)\n",
    "                print('precision@30', precision30)\n",
    "                \n",
    "                print('avg_precision', avg_precision)\n",
    "                print('lastrel', lastrel)\n",
    "                \n",
    "                \n",
    "            else: \n",
    "                recall10 = 0\n",
    "                recall20 = 0\n",
    "                recall30 = 0\n",
    "\n",
    "                precision10 = 0\n",
    "                precision20 = 0\n",
    "                precision30 = 0\n",
    "\n",
    "                avg_precision = average_precision(pred)\n",
    "                lastrel = 1\n",
    "                \n",
    "                \n",
    "            recalls10.append(recall10)\n",
    "            recalls20.append(recall20)\n",
    "            recalls30.append(recall30)\n",
    "\n",
    "            precisions10.append(precision10)\n",
    "            precisions20.append(precision20)\n",
    "            precisions30.append(precision30)\n",
    "\n",
    "            avg_precisions.append(avg_precision)\n",
    "            lastrels.append(lastrel)\n",
    "            \n",
    "            if counter == ITER:  # finish iterating with this query \n",
    "                break\n",
    "\n",
    "            query = modify_query(query_result, query, ORIGIN_QUERY)\n",
    "\n",
    "            counter +=1 \n",
    "        \n",
    "        total_results[QUERY_ID]['recall@10'] = recalls10\n",
    "        total_results[QUERY_ID]['recall@20'] = recalls20\n",
    "        total_results[QUERY_ID]['recall@30'] = recalls30\n",
    "        \n",
    "        total_results[QUERY_ID]['precision@10'] = precisions10\n",
    "        total_results[QUERY_ID]['precision@20'] = precisions20\n",
    "        total_results[QUERY_ID]['precision@30'] = precisions30\n",
    "        \n",
    "        total_results[QUERY_ID]['avg_prec'] = avg_precisions\n",
    "        total_results[QUERY_ID]['lastrel'] = lastrels\n",
    "        \n",
    "        \n",
    "    return total_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T16:59:17.663143Z",
     "start_time": "2019-12-10T16:58:53.401499Z"
    }
   },
   "outputs": [],
   "source": [
    "dict_results = main_rocchio()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T16:59:27.942790Z",
     "start_time": "2019-12-10T16:59:27.937215Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('relevance_feedback_rocchio_epistemonikos.json', 'w') as f:\n",
    "    json.dump(dict_results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# relevance feedback BM25 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T17:10:34.002993Z",
     "start_time": "2019-12-10T17:10:33.992865Z"
    }
   },
   "outputs": [],
   "source": [
    "def main_bm25():\n",
    "    total_results  = {}\n",
    "    \n",
    "    query_ids = [idx2query[i] for i in range(len(queries))]\n",
    "    \n",
    "    # start_result_dict\n",
    "    for id_ in query_ids: \n",
    "        total_results[id_] = {'recall@10':[], \n",
    "                              'recall@20':[],\n",
    "                              'recall@30':[],\n",
    "                              'precision@10':[],\n",
    "                             'precision@20':[],\n",
    "                              'precision@30':[],\n",
    "                              'avg_prec':[],\n",
    "                              'lastrel%':[]\n",
    "                             }\n",
    "    \n",
    "    \n",
    "    for i in range(len(queries)):\n",
    "        \n",
    "        results = {}\n",
    "    \n",
    "        ORIGIN_QUERY = queries[i]\n",
    "\n",
    "        ORIGIN_QUERY = ORIGIN_QUERY.lower()  # Record original query terms\n",
    "\n",
    "        query = ORIGIN_QUERY.rstrip('\\n').split()\n",
    "\n",
    "        RESULT_SIZE = 20000 \n",
    "        \n",
    "        ITER = 20 \n",
    "\n",
    "        QUERY_ID = idx2query[i]\n",
    "\n",
    "        counter = 0\n",
    "        \n",
    "        recalls10 = []\n",
    "        recalls20 = []\n",
    "        recalls30 = []\n",
    "        \n",
    "        precisions10 = []\n",
    "        precisions20 = []\n",
    "        precisions30 = []\n",
    "        \n",
    "        avg_precisions = []\n",
    "        lastrels = []\n",
    "        \n",
    "      \n",
    "        while True:\n",
    "\n",
    "            query_result = search(query, RESULT_SIZE)\n",
    "            \n",
    "            get_feedback(query_result, QUERY_ID)\n",
    "           \n",
    "            pred = [1 if q.is_relevant else 0 for q in query_result]\n",
    "            \n",
    "            if sum(pred) > 0: \n",
    "            \n",
    "                recall10 = sum(pred[0:10])/len(dict_episte_queries[QUERY_ID]['relevant_docs'])\n",
    "                recall20 = sum(pred[0:20])/len(dict_episte_queries[QUERY_ID]['relevant_docs'])\n",
    "                recall30 = sum(pred[0:30])/len(dict_episte_queries[QUERY_ID]['relevant_docs'])\n",
    "\n",
    "                precision10 = sum(pred[0:10])/10\n",
    "                precision20 = sum(pred[0:20])/20\n",
    "                precision30 = sum(pred[0:30])/30\n",
    "\n",
    "                avg_precision = average_precision(pred)\n",
    "                lastrel = last_rel(pred)\n",
    "                \n",
    "                print('recall@10', recall10)\n",
    "                print('recall@20', recall20)\n",
    "                print('recall@30', recall30)\n",
    "                \n",
    "                print('precision@10', precision10)\n",
    "                print('precision@20', precision20)\n",
    "                print('precision@30', precision30)\n",
    "                \n",
    "                print('avg_precision', avg_precision)\n",
    "                print('lastrel', lastrel)\n",
    "                \n",
    "                \n",
    "            else: \n",
    "                recall10 = 0\n",
    "                recall20 = 0\n",
    "                recall30 = 0\n",
    "\n",
    "                precision10 = 0\n",
    "                precision20 = 0\n",
    "                precision30 = 0\n",
    "\n",
    "                avg_precision = average_precision(pred)\n",
    "                lastrel = 1\n",
    "                \n",
    "                \n",
    "            recalls10.append(recall10)\n",
    "            recalls20.append(recall20)\n",
    "            recalls30.append(recall30)\n",
    "\n",
    "            precisions10.append(precision10)\n",
    "            precisions20.append(precision20)\n",
    "            precisions30.append(precision30)\n",
    "\n",
    "            avg_precisions.append(avg_precision)\n",
    "            lastrels.append(lastrel)\n",
    "            \n",
    "            if counter == ITER:  #  finish iterating with this query \n",
    "                break\n",
    "\n",
    "            counter +=1 \n",
    "        \n",
    "        total_results[QUERY_ID]['recall@10'] = recalls10\n",
    "        total_results[QUERY_ID]['recall@20'] = recalls20\n",
    "        total_results[QUERY_ID]['recall@30'] = recalls30\n",
    "        \n",
    "        total_results[QUERY_ID]['precision@10'] = precisions10\n",
    "        total_results[QUERY_ID]['precision@20'] = precisions20\n",
    "        total_results[QUERY_ID]['precision@30'] = precisions30\n",
    "        \n",
    "        total_results[QUERY_ID]['avg_prec'] = avg_precisions\n",
    "        total_results[QUERY_ID]['lastrel'] = lastrels\n",
    "        \n",
    "        \n",
    "    return total_results\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-12-10T17:09:02.446Z"
    }
   },
   "outputs": [],
   "source": [
    "dict_results_bm25 = main_bm25()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('relevance_feedback_bm25_epistemonikos.json', 'w') as f:\n",
    "    json.dump(dict_results_bm25, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
