{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Machine Learning Models from scratch using Active Learning on Epistemonikos dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-01T15:15:08.675399Z",
     "start_time": "2019-09-01T15:15:08.664213Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from utils import * \n",
    "\n",
    "import json \n",
    "import numpy as np\n",
    "import random \n",
    "import re\n",
    "import math\n",
    "import pandas as pd\n",
    "import pickle \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pickle \n",
    "from libact.models import LogisticRegression\n",
    "from libact.models.svm import SVM\n",
    "from libact.models import SklearnProbaAdapter\n",
    "from libact.labelers import IdealLabeler\n",
    "from libact.query_strategies import RandomSampling, UncertaintySampling\n",
    "from libact.base.dataset import Dataset, import_libsvm_sparse\n",
    "from libact.models.sklearn_adapter import SklearnAdapter\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# random seed for random shuffle \n",
    "seed = 100\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load document embeddings pre-trained/pre-calculated for this dataset\n",
    "- Choose desired embedding BERT, BioBERT, Word2Vec, GloVe or TF-IDF.\n",
    "- Comment/Uncomment the desired embedding dictionary for active learning.\n",
    "- We assume the .json files has been downloaded in the same folder as the script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-01T15:17:17.300892Z",
     "start_time": "2019-09-01T15:15:11.693414Z"
    }
   },
   "outputs": [],
   "source": [
    "DATASET_DIR = ''  # absolute path where dataset was downloaded and unzipped\n",
    "\n",
    "chosen_representation = 'TF-IDF' # choose 'GLOVE', 'W2VEC', 'BERT', 'BioBERT' or 'TF-IDF' \n",
    "\n",
    "dict_embeddings, embedding_dim = return_embeddings_episte(chosen_representation, DATASET_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## metaparameters for active learning setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_labeled = 5 # number of initial labeled documents  \n",
    "quota = 100 # total documents asked to the oracle \n",
    "batch = int(quota/10) # number of documents asked to the oracle on each iteration  \n",
    "\n",
    "# minimum and max number of relevant documents per question for epistemonikos dataset \n",
    "max_docs = 2200\n",
    "min_docs = 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load epistemonikos dataset\n",
    "- this dataset contains only relevant documents for each medical questions. \n",
    "- for active learning framework we choose relevant documents for each medical question and sample non relevant documents not linked to these questions. \n",
    "- We assume the Epistemonikos dataset has been downloaded in the same folder as the script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-01T15:17:21.247143Z",
     "start_time": "2019-09-01T15:17:20.862496Z"
    }
   },
   "outputs": [],
   "source": [
    "matrix_doc_df = pd.read_csv('{}/datasets/Epistemonikos_dataset.csv'.format(DATASET_DIR), sep=';')\n",
    "list_documents = list(dict_embeddings.keys())\n",
    "\n",
    "# consider only documents in dict embeddings \n",
    "matrix_doc_df = matrix_doc_df[matrix_doc_df.document.isin(list_documents)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## start active learning iterations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-01T15:25:43.887155Z",
     "start_time": "2019-09-01T15:23:20.480759Z"
    }
   },
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "# define models \n",
    "models = ['RF', 'LR', 'MLP', 'SVM_linear','SVM_rbf'] \n",
    "\n",
    "# filter medical questions depending on minimum and maximum documents per question chosen as meta-parameter \n",
    "matrices = [m for m in list(matrix_doc_df.matrix.unique()) if len(matrix_doc_df.loc[matrix_doc_df.matrix == m]) < max_docs and len(matrix_doc_df.loc[matrix_doc_df.matrix == m]) > min_docs]\n",
    "\n",
    "for model in models:\n",
    "    \n",
    "    machine_learning_model = return_model(model)\n",
    "        \n",
    "    final_results = {}\n",
    "     \n",
    "    for m in matrices: \n",
    "        final_results[m] = {'label':[], 'prediction': []}\n",
    "    \n",
    "    \n",
    "    for m in matrices:\n",
    "\n",
    "        relevants = matrix_doc_df.loc[matrix_doc_df.matrix == m]\n",
    "\n",
    "        # sample non relevant documents different from the actual question \n",
    "        non_relevants = matrix_doc_df.loc[matrix_doc_df.matrix != m].sample(\n",
    "            n=len(relevants)*20)  # non-rel 20 times the q of relevant\n",
    "\n",
    "        non_relevants.relevance = 0\n",
    "\n",
    "        matrix_concat = pd.concat([relevants, non_relevants])\n",
    "\n",
    "        # document vectors \n",
    "        vectors = [ ]\n",
    "\n",
    "        for doc_id in matrix_concat.document:\n",
    "            vectors.append(dict_embeddings[doc_id])\n",
    "\n",
    "        labels = [int(x) for x in matrix_concat.relevance]\n",
    "\n",
    "        # random shuffle data \n",
    "        c = list(zip(vectors, labels))\n",
    "        random.shuffle(c)\n",
    "        vectores, labels = zip(*c)\n",
    "\n",
    "        # get dataset with observations without tags (None) and labeled ones for ground-truth\n",
    "        X, y,X_test, y_test, ds_unlabeled, fully_labeled_ds = dataset_preprocesing(vectors, labels, n_labeled)\n",
    "\n",
    "        # ideal labeler that pulls ground truth labels\n",
    "        lbr = IdealLabeler(fully_labeled_ds)\n",
    "\n",
    "\n",
    "        # choose active learning strategy \n",
    "        qs = UncertaintySampling(ds_unlabeled, model=machine_learning_model)\n",
    "        #qs = RandomSampling(ds_unlabeled)\n",
    "\n",
    "\n",
    "        for i in range(quota):                                     \n",
    "\n",
    "            ask_id = qs.make_query()\n",
    "            X, labels_new = zip(*ds_unlabeled.data)\n",
    "            lb = lbr.label(X[ask_id])\n",
    "            ds_unlabeled.update(ask_id, lb)\n",
    "            machine_learning_model.train(ds_unlabeled)\n",
    "\n",
    "            y_prob = [x[1] for x in machine_learning_model.predict_proba(X_test)]\n",
    "\n",
    "            # after 10 iterations store prediction results and actual labels\n",
    "            if i%10 == 0:\n",
    "                final_results[m]['label'].append(y_test)\n",
    "                final_results[m]['prediction'].append(y_prob)\n",
    "                \n",
    "                print(final_results)\n",
    "                \n",
    "    # save model \n",
    "    print('saving model ...')\n",
    "    pickle.dump(machine_learning_model, open('{}{}.sav'.format(model, chosen_representation), 'wb'))\n",
    "\n",
    "   \n",
    "     # save results \n",
    "    print('saving results...')\n",
    "    with open('results_{}_{}.json'.format(model, chosen_representation), 'w') as fp:\n",
    "        json.dump(final_results,fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
